{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import json\n",
    "import boto3\n",
    "import lancedb\n",
    "\n",
    "# load data.json, can download from \n",
    "# https://raw.githubusercontent.com/banool/auslan_dictionary/master/assets/data/data.json\n",
    "with open('data.json') as f:\n",
    "    json_data = json.load(f)\n",
    "\n",
    "json_data = json_data[\"data\"]\n",
    "embed_df = pd.json_normalize(json_data,\n",
    "                            ['sub_entries'],\n",
    "                            ['entry_in_english'],\n",
    "                            record_prefix='sub_entries_',\n",
    "                            max_level=0)\n",
    "embed_df = embed_df.drop(columns=['sub_entries_regions'])\n",
    "embed_df = embed_df.drop(columns=['sub_entries_video_links'])\n",
    "embed_df = embed_df.drop(columns=['sub_entries_keywords'])\n",
    "embed_df['definitions'] = embed_df['sub_entries_definitions'].apply(lambda x: ';'.join([f\"{k}: {'; '.join(v)}\" for k, v in x.items()]))\n",
    "embed_df = embed_df.drop(columns=['sub_entries_definitions'])\n",
    "embed_df['word_id'] = pd.factorize(embed_df['entry_in_english'])[0] + 1\n",
    "embed_df = embed_df.rename(columns={'entry_in_english': 'word'})\n",
    "embed_df.to_csv('embed_df.csv', index=False)\n",
    "\n",
    "embed_test_df = embed_df.head(10)\n",
    "embed_test_df.to_csv('embed_test_df.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# AWS\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()\n",
    "# from .env file\n",
    "# AWS_ACCESS_KEY_ID=your_aws_access_key_id\n",
    "# AWS_SECRET_ACCESS_KEY=your_aws_secret_access_key\n",
    "# AWS_DEFAULT_REGION=\"ap-southeast-2\"\n",
    "\n",
    "\n",
    "session = boto3.Session(profile_name='auslan', region_name='ap-southeast-2')\n",
    "s3 = boto3.client('s3')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pydantic schema\n",
    "from langchain_community.document_loaders.csv_loader import CSVLoader\n",
    "from langchain_aws.embeddings import BedrockEmbeddings\n",
    "from langchain_community.vectorstores import LanceDB\n",
    "\n",
    "bucket_name = \"auslan-data-lake-bucket\"\n",
    "prefix = \"silver\"\n",
    "object_key = f\"{prefix}/vectorized\"\n",
    "uri = f\"s3://{bucket_name}/{object_key}\"\n",
    "\n",
    "# connect locally to any path for the embedded db, i.e. uri=\"./langchain\"\n",
    "db_conn = lancedb.connect(uri=uri, region=\"ap-southeast-2\")\n",
    "\n",
    "# change to embed_df.csv for full dataset\n",
    "loader = CSVLoader('embed_test_df.csv', metadata_columns=['word_id'])\n",
    "\n",
    "docs = loader.load()\n",
    "\n",
    "bedrock_client = boto3.client(\n",
    "    'bedrock-runtime',\n",
    "    \"ap-southeast-2\",\n",
    ")\n",
    "\n",
    "embeddings = BedrockEmbeddings(\n",
    "    model_id='amazon.titan-embed-text-v2:0', \n",
    "    client=bedrock_client\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# run once\n",
    "db = LanceDB.from_documents(docs, embeddings, connection=db_conn, table_name=\"vectorstore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get table values (first conn)\n",
    "table=db._table\n",
    "table_df = table.to_pandas().head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# second connection\n",
    "db2 = lancedb.connect(uri=uri)\n",
    "table = db2.open_table(\"vectorstore\") # default table name\n",
    "\n",
    "# full text search index\n",
    "# table.create_index(metric=\"cosine\", num_sub_vectors=128, replace=True)\n",
    "table.create_index(metric=\"cosine\", num_sub_vectors=128, index_type=\"IVF_HNSW_PQ\", replace=True,)\n",
    "table.create_fts_index(\"text\", use_tantivy=False, language=\"English\", stem=True, replace=True, with_position=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from lancedb.rerankers import LinearCombinationReranker\n",
    "\n",
    "# reranking for hybrid search\n",
    "reranker = LinearCombinationReranker(weight=0.7)\n",
    "\n",
    "event_json = {\n",
    "  \"body\": \"value1\",\n",
    "}\n",
    "\n",
    "# get the query string from the event\n",
    "query = event_json.get(\"body\", \"{}\")\n",
    "print(query)\n",
    "\n",
    "# example search query\n",
    "embedded_query = embeddings.embed_query(query)\n",
    "print(embedded_query)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# rerank limit not based on the row but the metadata's word_id\n",
    "res = table.search(query_type=\"hybrid\").rerank(reranker=reranker).vector(query).text(\"test query\").limit(20).to_pandas()\n",
    "print(res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter top 10 unique words\n",
    "unique_res = pd.json_normalize(res[\"metadata\"])\n",
    "unique_res = unique_res.drop_duplicates(subset=['word_id']).head(5)\n",
    "unique_res = unique_res.drop(columns=['source','row'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "out = unique_res.to_json()\n",
    "out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'ResponseMetadata': {'RequestId': 'W1HDHDJ17KDVWSWP',\n",
       "  'HostId': 'aHxYdRXkByup5iBCBkwW7F8r4QZOlPppIo5QvHDVAVX2R8hH/6gXigq+YKc/HIcK+vILPH6le98wMKvjiAQb6jSmTnYx0jh0kSFE0bF1DH4=',\n",
       "  'HTTPStatusCode': 200,\n",
       "  'HTTPHeaders': {'x-amz-id-2': 'aHxYdRXkByup5iBCBkwW7F8r4QZOlPppIo5QvHDVAVX2R8hH/6gXigq+YKc/HIcK+vILPH6le98wMKvjiAQb6jSmTnYx0jh0kSFE0bF1DH4=',\n",
       "   'x-amz-request-id': 'W1HDHDJ17KDVWSWP',\n",
       "   'date': 'Thu, 28 Nov 2024 12:15:28 GMT',\n",
       "   'x-amz-version-id': 'uIz_hAUn3JzrxbD5qzl7eU.7wvuxMXBv',\n",
       "   'x-amz-server-side-encryption': 'AES256',\n",
       "   'etag': '\"4b9640e83a03f959d6f1c0c0d56a3c41\"',\n",
       "   'content-length': '0',\n",
       "   'server': 'AmazonS3'},\n",
       "  'RetryAttempts': 0},\n",
       " 'ETag': '\"4b9640e83a03f959d6f1c0c0d56a3c41\"',\n",
       " 'ServerSideEncryption': 'AES256',\n",
       " 'VersionId': 'uIz_hAUn3JzrxbD5qzl7eU.7wvuxMXBv'}"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import io\n",
    "\n",
    "bucket_name = \"auslan-data-lake-bucket\"\n",
    "\n",
    "dest_object_key = \"gold/embeddings.csv\"\n",
    "\n",
    "with open('data.json') as f:\n",
    "    json_data = json.load(f)\n",
    "\n",
    "json_data = json_data[\"data\"]\n",
    "df = pd.json_normalize(\n",
    "    json_data,\n",
    "    [\"sub_entries\"],\n",
    "    [\"entry_in_english\"],\n",
    "    record_prefix=\"sub_entries_\",\n",
    "    max_level=0,\n",
    ")\n",
    "df[\"definitions\"] = df[\"sub_entries_definitions\"].apply(\n",
    "    lambda x: \";\".join([f\"{k}: {'; '.join(v)}\" for k, v in x.items()])\n",
    ")\n",
    "df = df.drop(\n",
    "    columns=[\n",
    "        \"sub_entries_regions\",\n",
    "        \"sub_entries_video_links\",\n",
    "        \"sub_entries_keywords\",\n",
    "        \"sub_entries_definitions\",\n",
    "    ]\n",
    ")\n",
    "df = df.rename(columns={\"entry_in_english\": \"word\"})\n",
    "df[\"word_id\"] = pd.factorize(df[\"word\"])[0] + 1\n",
    "\n",
    "csv_buffer = io.StringIO()\n",
    "df.to_csv(csv_buffer, index=False)\n",
    "\n",
    "s3.put_object(\n",
    "    Bucket=bucket_name,\n",
    "    Key=dest_object_key,\n",
    "    Body=csv_buffer.getvalue(),\n",
    ")\n",
    "\n",
    "# Parquet file for final data retrieval\n",
    "db_df = pd.json_normalize(json_data, max_level=1)\n",
    "db_df[\"word_id\"] = pd.factorize(db_df[\"entry_in_english\"])[0] + 1\n",
    "\n",
    "# convert columns to string\n",
    "columns_to_convert = ['sub_entries', 'entry_type', 'categories', 'entry_in_english']\n",
    "for column in columns_to_convert:\n",
    "    db_df[column] = db_df[column].astype(str)\n",
    "\n",
    "parquet_buffer = io.BytesIO()\n",
    "db_df.to_parquet(parquet_buffer, index=False)\n",
    "\n",
    "s3.put_object(\n",
    "    Bucket=bucket_name,\n",
    "    Key=\"gold/dictionary.parquet\",\n",
    "    Body=parquet_buffer.getvalue(),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "rs = [1, 2, 3, 4, 5]\n",
    "parquet_object_key = \"gold/dictionary.parquet\"\n",
    "\n",
    "def load_parquet_to_df() -> pd.DataFrame:\n",
    "    obj = s3.get_object(Bucket=bucket_name, Key=parquet_object_key)\n",
    "    parquet_data = io.BytesIO(obj[\"Body\"].read())\n",
    "    df = pd.read_parquet(parquet_data)\n",
    "    return df\n",
    "\n",
    "new_df = load_parquet_to_df()\n",
    "filtered_df = new_df[new_df[\"word_id\"].isin(rs)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>entry_in_english</th>\n",
       "      <th>sub_entries</th>\n",
       "      <th>entry_type</th>\n",
       "      <th>categories</th>\n",
       "      <th>word_id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>a, A</td>\n",
       "      <td>[{'video_links': ['https://media.auslan.org.au...</td>\n",
       "      <td>WORD</td>\n",
       "      <td>[]</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>abattoir</td>\n",
       "      <td>[{'video_links': ['https://object-store.rc.nec...</td>\n",
       "      <td>WORD</td>\n",
       "      <td>['Work']</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>abbreviate</td>\n",
       "      <td>[{'video_links': ['https://object-store.rc.nec...</td>\n",
       "      <td>WORD</td>\n",
       "      <td>['Education']</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>abbreviation</td>\n",
       "      <td>[{'video_links': ['https://object-store.rc.nec...</td>\n",
       "      <td>WORD</td>\n",
       "      <td>['Education']</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>abdomen</td>\n",
       "      <td>[{'video_links': ['https://object-store.rc.nec...</td>\n",
       "      <td>WORD</td>\n",
       "      <td>['Health', 'Body Parts']</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  entry_in_english                                        sub_entries  \\\n",
       "0             a, A  [{'video_links': ['https://media.auslan.org.au...   \n",
       "1         abattoir  [{'video_links': ['https://object-store.rc.nec...   \n",
       "2       abbreviate  [{'video_links': ['https://object-store.rc.nec...   \n",
       "3     abbreviation  [{'video_links': ['https://object-store.rc.nec...   \n",
       "4          abdomen  [{'video_links': ['https://object-store.rc.nec...   \n",
       "\n",
       "  entry_type                categories  word_id  \n",
       "0       WORD                        []        1  \n",
       "1       WORD                  ['Work']        2  \n",
       "2       WORD             ['Education']        3  \n",
       "3       WORD             ['Education']        4  \n",
       "4       WORD  ['Health', 'Body Parts']        5  "
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "filtered_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from lancedb.pydantic import LanceModel\n",
    "from typing import List, Dict\n",
    "\n",
    "db_df = pd.json_normalize(json_data, max_level=1)\n",
    "db_df[\"word_id\"] = pd.factorize(db_df[\"entry_in_english\"])[0] + 1\n",
    "db_df.to_csv('file_size.csv')\n",
    "# convert 'word_id' column to str\n",
    "rs=unique_res['word_id'].astype(int).to_list()\n",
    "filtered_df = db_df[db_df['word_id'].isin(rs)]\n",
    "print(json.dumps(filtered_df.to_dict(orient=\"records\"), indent=2))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
